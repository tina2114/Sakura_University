### cpu虚拟化介绍

intel在X86 CPU的基础上增加VMX架构来实现CPU的硬件虚拟化，VMX架构下有两类角色：虚拟机监视器（VMM）和虚拟机（VM）。VMM对整个系统的CPU和硬件有完全的控制权，它抽象出虚拟的CPU给各个VM，并能够将VM的CPU直接调度到物理CPU上运行。VM是一个虚拟机实例，能够支持操作系统以及各种软件栈和应用程序。

在QEMU中，QEMU/KVM是作为VMM，你挂载的虚拟机镜像作为VM。QEMU负责模拟整个电脑，构建对应架构，但是其中内存和CPU的虚拟化工作由KVM进行负责。

#### VMCS介绍

VMCS用来管理VMX non-root Operation的转换以及控制VCPU的行为。VMCS之于VCPU的作用类似于进程描述符之于进程的作用。

在VMCS的格式中，前8个字节是固定的，第一个4字节的第0位到第30位表示修正标识符，用来识别不同的VMCS版本，第一个4字节的第31位是shadow-VMCS indicator，VMM根据这个来判断是一个普通的VMCS还是shadow VMCS[^shadow]。VMCS的第二个4字节是VMX-abort indicator，当VM Exit发生错误时，会产生VMX-abort，导致处理器进入关闭，处理器写入一个非零值到VMX-abort indicator

[^shadow]: VMCS SHADOW允许硬件加速vmread，vmwrite指令，它允许VMM不拦截VMCS读/写的某些字段

VMCS数据区控制着VMX non-root和VMX root之间的转换，VMM通过VMREAD和VMWRITE指令在这里读写。一共6个数据区：

1. Guest-state区域，进行VM Entry时，虚拟机处理器的状态信息从这个区域加载，进行VM Exit的时候，虚拟机的当前状态信息写入到这个区域
2. Host-state区域，发生VM Exit的时候，需要切换到VMM的上下文运行，此时处理器的状态信息从这里加载
3. VM-execution控制区域，用来控制处理器在进入VM Entry之后的处理器行为
4. VM Exit控制区域，用来指定虚拟机在发生VM Exit时的行为，如一些寄存器的保存
5. VM Entry控制区域，指定虚拟机在发生VM Entry时的行为，如一些寄存器的加载
6. VM Exit信息区域，包含了最近产生的VM Exit信息，典型的信息包括退出的原因以及相应的数据

VCPU之间会共享物理CPU，VMM负责在多个VCPU之间分配物理CPU，每个VCPU都有自己的描述符，当VMM在切换VCPU运行时需要保存此时VCPU的状态。

### KVM模块初始化

KVM是基于内核的虚拟机监视器

KVM模块的初始化主要包括初始化CPU与架构无关的数据以及设置与架构有关的虚拟化支持。

VMM只有在CPU处于保护模式并且开启分页时才能进入VMX模式，以下是开启VMX模式需要做的事情：

1. 使用CPUID检测CPU是否支持VMX
2. 检测CPU支持VMX的能力，通过读写与VMX能力相关的MSR寄存器完成的
3. 分配一段4KB对齐的内存作为VMXON区域
4. 初始化VMXON区域的版本标识
5. 确保当前CPU运行模式的CR0寄存器符合进入VMX的条件，如CR0.PE = 1，CR0.PG = 1
6. 通过设置CR4.VMXE为1来开启VMX模式
7. 确保IA32_FEATURE_CONTROL寄存器被正确设置，其锁定位（0位）为1
8. 使用VMXON区域的物理地址作为操作数调用VMXON指令，执行完成后，如果RFLAGS.CF = 0则表示指令执行成功

KVM初始化既要配置好上述架构相关数据，也要完成架构无关部分，这部分由`kvm_init`完成

![image-20201019212350748](C:\Users\zhz\AppData\Roaming\Typora\typora-user-images\image-20201019212350748.png)

+ kvm_arch_init函数用来初始化架构相关代码，确保只有一个KVM实现能够加载到内核
+ kvm_irqfd_init初始化irqfd相关的数据，主要是创建一个线程
+ kvm_arch_hardware_setup创建一些跟启动KVM密切相关的数据结构以及初始化一些硬件特性
+ kvm_arch_check_processor_compat检测所有CPU的特性是否一致
+ 最后一个重要工作是创建一个Misc设备“/dev/kvm"
  - “/dev/kvm"的接口分为两类：一类为通用接口，如KVM_API_VERSION和KVM_CREATE_VM；另一类为架构相关接口。
  - KVM的ioctl处理整个KVM层面的请求

总结一下就是：KVM模块的初始化过程主要是对硬件进行检查，分配一些常用结构的缓存，创建一个“/dev/kvm"设备，得到vmcs的一个配置结构vmcs_config，并根据CPU特性设置一些全局变量，给每个物理CPU分配一个vmcs结构。注意，此时CPU还不在VMX模式下。因为即使创建了KVM模块，但是没有虚拟机的创建，那也就不必要让CPU进入VMX模式。

### 虚拟机的创建

从QEMU和KVM两个方向来考察KVM虚拟机的创建

#### QEMU

QEMU中使用KVMState结构体来表示KVM相关的数据结构。KVM_INIT函数首先打开”/dev/kvm"设备得到一个fd，并且会保存到类型为KVMState的变量s的成员fd中。调用ioctl(KVM_CREATE_VM)接口在KVM层面创建一个虚拟机

#### KVM

kvm_init最重要的作用就是调用“/dev/kvm"设备的ioclt(KVM_CREATE_VM)接口，在KVM模块中创建一台虚拟机。

`kvm_create_vm`是创建虚拟机的核心函数：

+ kvm_arch_alloc_vm分配一个KVM结构体，用来表示一台虚拟机
+ kvm_arch_init_vm初始化与架构相关的数据
+ hardware_enable_all来最终开启VMX模式，其主要调用hardware_enable函数，这函数设置CR4的VMXE位并且调用VMXON指令开启VMX
+ kvm_alloc_memslots为虚拟机分配内存槽[^1]
+ 为KVM结构体中类型为kvm_io_bus的成员buses分配空间，其作用是将内核中实现的模拟设备连接起来

[^1]: [内存](https://baike.baidu.com/item/内存)插槽是指[主板](https://baike.baidu.com/item/主板/104636)上用来插[内存](https://baike.baidu.com/item/内存)条的插槽

### CPU的创建

QEMU能模拟多种CPU，所以存在一套继承结构，如下图：

![image-20201019223220033](https://gitee.com/zhzzhz/blog_warehouse/raw/master/img/image-20201019223220033.png)

这里拿X86来举例，QEMU支持的x86CPU都定义在一个builtin_x86_deffs数组中，该数组的类型是X86CPUDefinition

```c
struct X86CPUDefinition {
    const char *name;
    uint32_t level;	//  CPUID指令支持的最大功能号
    uint32_t xlevel; // CPUID扩展质量支持的最大功能号
    uint32_t xlevel2;
    /* vendor is zero-terminated, 12 character ASCII string */
    char vendor[CPUID_VENDOR_SZ + 1]; // cpu的基本信息
    int family; // cpu的基本信息
    int model; // cpu的基本信息
    int stepping; // cpu的基本信息
    FeatureWordArray features; // 记录CPU特性的数组
    char model_id[48]; // CPU全名
    bool cache_info_passthrough;
};
```

TYPE_X86_CPU的对象实例由X86CPU表示，其保存了CPU的各种信息。

一个X86CPU表示一个X86虚拟CPU。X86结构体中的CPUState成员表示所有CPU通用数据，CPUX86State表示X86CPU的数据（通用寄存器，eip，eflaghs段寄存器，KVM相关的异常和中断信息以及CPUID的信息）

X86CPU，CPUState，CPUX86State三者的关系如下图：

![image-20201019225347284](https://gitee.com/zhzzhz/blog_warehouse/raw/master/img/image-20201019225347284.png)

#### cpu对象初始化

设备具现化时会最先执行DeviceClass的具现函数，这里直接调用了x86_cpu_realizefn

```c
static void x86_cpu_common_class_init(ObjectClass *oc, void *data)
{
    X86CPUClass *xcc = X86_CPU_CLASS(oc);
    CPUClass *cc = CPU_CLASS(oc);
    DeviceClass *dc = DEVICE_CLASS(oc);

    xcc->parent_realize = dc->realize;
    xcc->parent_unrealize = dc->unrealize;
    dc->realize = x86_cpu_realizefn;
    dc->unrealize = x86_cpu_unrealize;
}
```

TYPE_X86_CPU的初始化函数是`x86_cpu_initfn`

```c
static void x86_cpu_initfn(Object *obj)
{
    CPUState *cs = CPU(obj);
    X86CPU *cpu = X86_CPU(obj);
    X86CPUClass *xcc = X86_CPU_GET_CLASS(obj);
    CPUX86State *env = &cpu->env;
    static int inited;

    cs->env_ptr = env; // 把X86CPUState的地址赋给了CPUState的env_ptr，这样基类的env就直接指向了实例化的x86cpu的env
    cpu_exec_init(env);
	// 一系列add函数，给这个CPU对象添加属性，属性表示的是CPU的基本信息
    object_property_add(obj, "family", "int",
                        x86_cpuid_version_get_family,
                        x86_cpuid_version_set_family, NULL, NULL, NULL);
    object_property_add(obj, "model", "int",
                        x86_cpuid_version_get_model,
                        x86_cpuid_version_set_model, NULL, NULL, NULL);
    object_property_add(obj, "stepping", "int",
                        x86_cpuid_version_get_stepping,
                        x86_cpuid_version_set_stepping, NULL, NULL, NULL);
    object_property_add(obj, "level", "int",
                        x86_cpuid_get_level,
                        x86_cpuid_set_level, NULL, NULL, NULL);
    object_property_add(obj, "xlevel", "int",
                        x86_cpuid_get_xlevel,
                        x86_cpuid_set_xlevel, NULL, NULL, NULL);
    object_property_add_str(obj, "vendor",
                            x86_cpuid_get_vendor,
                            x86_cpuid_set_vendor, NULL);
    object_property_add_str(obj, "model-id",
                            x86_cpuid_get_model_id,
                            x86_cpuid_set_model_id, NULL);
    object_property_add(obj, "tsc-frequency", "int",
                        x86_cpuid_get_tsc_freq,
                        x86_cpuid_set_tsc_freq, NULL, NULL, NULL);
    object_property_add(obj, "apic-id", "int",
                        x86_cpuid_get_apic_id,
                        x86_cpuid_set_apic_id, NULL, NULL, NULL);
    object_property_add(obj, "feature-words", "X86CPUFeatureWordInfo",
                        x86_cpu_get_feature_words,
                        NULL, NULL, (void *)env->features, NULL);
    object_property_add(obj, "filtered-features", "X86CPUFeatureWordInfo",
                        x86_cpu_get_feature_words,
                        NULL, NULL, (void *)cpu->filtered_features, NULL);

    cpu->hyperv_spinlock_attempts = HYPERV_SPINLOCK_NEVER_RETRY;

#ifndef CONFIG_USER_ONLY
    /* Any code creating new X86CPU objects have to set apic-id explicitly */
    cpu->apic_id = -1;
#endif

    x86_cpu_load_def(cpu, xcc->cpu_def, &error_abort);

    /* init various static tables used in TCG mode */
    if (tcg_enabled() && !inited) {
        inited = 1;
        optimize_flags_init();
    }
}
```

CPU类型初始化(CPUState)，对象实例化(X86CPU)后，还需要具现化才能让CPU对象可用。`x86_cpu_realizfn`列举了CPU对象具现化的一些重要过程（这里的具现化应该就是调用realize函数）

```c
static void x86_cpu_realizefn(DeviceState *dev, Error **errp)
{
    CPUState *cs = CPU(dev);
    X86CPU *cpu = X86_CPU(dev);
    X86CPUClass *xcc = X86_CPU_GET_CLASS(dev);
    CPUX86State *env = &cpu->env;
    Error *local_err = NULL;
    static bool ht_warned;

    if (cpu->apic_id < 0) {
        error_setg(errp, "apic-id property was not initialized properly");
        return;
    }

    if (env->features[FEAT_7_0_EBX] && env->cpuid_level < 7) {
        env->cpuid_level = 7;
    }

    /* On AMD CPUs, some CPUID[8000_0001].EDX bits must match the bits on
     * CPUID[1].EDX.
     */
    if (IS_AMD_CPU(env)) {
        env->features[FEAT_8000_0001_EDX] &= ~CPUID_EXT2_AMD_ALIASES;
        env->features[FEAT_8000_0001_EDX] |= (env->features[FEAT_1_EDX]
           & CPUID_EXT2_AMD_ALIASES);
    }

	// x86_cpu_filter_features检查宿主机CPU特性能否支持创建的CPU对象
    if (x86_cpu_filter_features(cpu) && cpu->enforce_cpuid) {
        error_setg(&local_err,
                   kvm_enabled() ?
                       "Host doesn't support requested features" :
                       "TCG doesn't support requested features");
        goto out;
    }

#ifndef CONFIG_USER_ONLY
    qemu_register_reset(x86_cpu_machine_reset_cb, cpu);

    if (cpu->env.features[FEAT_1_EDX] & CPUID_APIC || smp_cpus > 1) {
        x86_cpu_apic_create(cpu, &local_err);
        if (local_err != NULL) {
            goto out;
        }
    }
#endif

    mce_init(cpu);
    // 根据QEMU使用的加速器来执行对应的CPU初始化函数
    qemu_init_vcpu(cs);

    /* Only Intel CPUs support hyperthreading. Even though QEMU fixes this
     * issue by adjusting CPUID_0000_0001_EBX and CPUID_8000_0008_ECX
     * based on inputs (sockets,cores,threads), it is still better to gives
     * users a warning.
     *
     * NOTE: the following code has to follow qemu_init_vcpu(). Otherwise
     * cs->nr_threads hasn't be populated yet and the checking is incorrect.
     */
    if (!IS_INTEL_CPU(env) && cs->nr_threads > 1 && !ht_warned) {
        error_report("AMD CPU doesn't support hyperthreading. Please configure"
                     " -smp options properly.");
        ht_warned = true;
    }

    x86_cpu_apic_realize(cpu, &local_err);
    if (local_err != NULL) {
        goto out;
    }
    cpu_reset(cs);

    xcc->parent_realize(dev, &local_err);
out:
    if (local_err != NULL) {
        error_propagate(errp, local_err);
        return;
    }
}
```

KVM下会调用qemu_kvm_start_vcpu创建VCPU线程，对每一个CPU对象会创建一个线程，名字为“CPU <id>/KVM”

VCPU的线程函数是`qemu_kvm_cpu_thread_fn`

```c
static void *qemu_kvm_cpu_thread_fn(void *arg)
{
    CPUState *cpu = arg;
    int r;

    qemu_mutex_lock(&qemu_global_mutex);
    qemu_thread_get_self(cpu->thread);
    cpu->thread_id = qemu_get_thread_id();
    cpu->can_do_io = 1;
    current_cpu = cpu;
	// 在kvm中创建VCPU
    r = kvm_init_vcpu(cpu);
    if (r < 0) {
        fprintf(stderr, "kvm_init_vcpu failed: %s\n", strerror(-r));
        exit(1);
    }
	// 初始化CPU的信号处理
    qemu_kvm_init_cpu_signals(cpu);

    /* signal CPU creation */
    cpu->created = true;
    qemu_cond_signal(&qemu_cpu_cond);
	// 判断CPU能否运行
    while (1) {
        if (cpu_can_run(cpu)) {
            r = kvm_cpu_exec(cpu); // 该函数调用KVM的VCPU的ioctl(KVM_RUN)，让VCPU在物理CPU上运行
            if (r == EXCP_DEBUG) {
                cpu_handle_guest_debug(cpu);
            }
        }
        qemu_kvm_wait_io_event(cpu);
    }

    return NULL;
}

```

继续关注`kvm_cpu_exec`函数

该函数主要就是一个do-while循环，在do中先让虚拟CPU在物理CPU上运行，然后应用层就阻塞在了此处，当虚拟机产生VM Exit时，内核就根据退出信息来进行处理，如此循环往复，完成CPU的虚拟化

```c
int kvm_cpu_exec(CPUState *cpu)
{
    struct kvm_run *run = cpu->kvm_run;
    int ret, run_ret;
	...

    do {
        ......
        // 让虚拟CPU在物理CPU上运行
        run_ret = kvm_vcpu_ioctl(cpu, KVM_RUN, 0);

        qemu_mutex_lock_iothread();
        kvm_arch_post_run(cpu, run);

        if (run_ret < 0) {
            if (run_ret == -EINTR || run_ret == -EAGAIN) {
                DPRINTF("io window exit\n");
                ret = EXCP_INTERRUPT;
                break;
            }
            fprintf(stderr, "error: kvm run failed %s\n",
                    strerror(-run_ret));
            ret = -1;
            break;
        }
	    // 当产生VM Exit时，内核根据退出的原因进行处理
        trace_kvm_run_exit(cpu->cpu_index, run->exit_reason);
        switch (run->exit_reason) {
        case KVM_EXIT_IO:
            DPRINTF("handle_io\n");
            kvm_handle_io(run->io.port,
                          (uint8_t *)run + run->io.data_offset,
                          run->io.direction,
                          run->io.size,
                          run->io.count);
            ret = 0;
            break;
        case KVM_EXIT_MMIO:
            DPRINTF("handle_mmio\n");
            cpu_physical_memory_rw(run->mmio.phys_addr,
                                   run->mmio.data,
                                   run->mmio.len,
                                   run->mmio.is_write);
            ret = 0;
            break;
        case KVM_EXIT_IRQ_WINDOW_OPEN:
            DPRINTF("irq_window_open\n");
            ret = EXCP_INTERRUPT;
            break;
        case KVM_EXIT_SHUTDOWN:
            DPRINTF("shutdown\n");
            qemu_system_reset_request();
            ret = EXCP_INTERRUPT;
            break;
        case KVM_EXIT_UNKNOWN:
            fprintf(stderr, "KVM: unknown exit, hardware reason %" PRIx64 "\n",
                    (uint64_t)run->hw.hardware_exit_reason);
            ret = -1;
            break;
        case KVM_EXIT_INTERNAL_ERROR:
            ret = kvm_handle_internal_error(cpu, run);
            break;
        case KVM_EXIT_SYSTEM_EVENT:
            switch (run->system_event.type) {
            case KVM_SYSTEM_EVENT_SHUTDOWN:
                qemu_system_shutdown_request();
                ret = EXCP_INTERRUPT;
                break;
            case KVM_SYSTEM_EVENT_RESET:
                qemu_system_reset_request();
                ret = EXCP_INTERRUPT;
                break;
            default:
                DPRINTF("kvm_arch_handle_exit\n");
                ret = kvm_arch_handle_exit(cpu, run);
                break;
            }
            break;
        default:
            DPRINTF("kvm_arch_handle_exit\n");
            ret = kvm_arch_handle_exit(cpu, run);
            break;
        }
    } while (ret == 0);

    if (ret < 0) {
        cpu_dump_state(cpu, stderr, fprintf, CPU_DUMP_CODE);
        vm_stop(RUN_STATE_INTERNAL_ERROR);
    }

    cpu->exit_request = 0;c
    return ret;
}
```

#### QEMU与KVM之间的共享数据

在KVM层面创建出VCPU，获取到代表该VCPU的fd后，就在"dev/kvm"设备的fd上调用ioctl(KVM_GET_VCPU_MMAP_SIZE)，该函数返回了KVM和QEMU共享空间的大小

返回值可能是1 or 2 or 3页，第一页用于kvm_run，该结构体用于QEMU和KVM进行基本的数据交互，第二页用于虚拟机访问IO端口时存储相应的数据，最后一页用于聚合的MMIO。

#### VCPU的运行

先来学习一个VMX指令集

| 指令     | 作用                                   |
| -------- | -------------------------------------- |
| VMPTRLD  | 加载一个VMCS结构体指针作为当前操作对象 |
| VMPTRST  | 保存当前VMCS结构体指针                 |
| VMCLEAR  | 清除当前VMCS结构体                     |
| VMREAD   | 读VMCS结构体指定域                     |
| VMWRITE  | 写VMCS结构体指定域                     |
| VMCALL   | 引发一个VMExit事件，返回到VMM          |
| VMLAUNCH | 启动一个虚拟机                         |
| VMRESUME | 从VMM返回到虚拟机继续运行              |
| VMXOFF   | 退出VMX操作模式                        |
| VMXON    | 进入VMX操作模式                        |

VMCX与VCPU密切相关，且每个VCPU都有一个对应的VMCS，故此此处介绍一下VMCS的四种状态：

1. Inactive：只是分配和初始化VMCS结构或者执行VMCLEAR指令之后的状态
2. working：CPU在一个VMCS上执行了VMPTRLD指令或者产生VM Exit之后所处的状态，这个时候CPU还是root状态
3. Active：当前VMCS执行了VMPTRLD指令，同一个CPU执行了另一个VCPU的VMPTRLD之后，前一个VMCS所处的状态
4. controlling：当CPU在一个VMCS上执行了MLAUNCH指令之后CPU所处的VMX non-root状态

VCPU执行的核心函数是`kvm_cpu_exec`，也同样是一个do-while的循环。

1. 触发VCPU的ioctl(KVM_RUN)使该CPU运行起来，KVM模块在处理该ioctl时，会执行对应的VMX指令，把该VCPU运行的CPU从VMX root模式转换成VMX non-root模式，开始运行虚拟机中的代码。
2. 虚拟机内部如果遇到一些事件产生VM Exit，就会退出到KVM，如果KVM无法处理就分发到QEMU进行初步处理，然后根据QEMU和KVM共享内存kvm_run中的数据判断原因，做出相应处理

```c
int kvm_cpu_exec(CPUState *cpu)
{
    struct kvm_run *run = cpu->kvm_run;
    ......

    do {
        if (cpu->kvm_vcpu_dirty) {
            kvm_arch_put_registers(cpu, KVM_PUT_RUNTIME_STATE);
            cpu->kvm_vcpu_dirty = false;
        }

        kvm_arch_pre_run(cpu, run);
        if (cpu->exit_request) {
            DPRINTF("interrupt exit requested\n");
            /*
             * KVM requires us to reenter the kernel after IO exits to complete
             * instruction emulation. This self-signal will ensure that we
             * leave ASAP again.
             */
            qemu_cpu_kick_self();
        }
        qemu_mutex_unlock_iothread();

        run_ret = kvm_vcpu_ioctl(cpu, KVM_RUN, 0);

        qemu_mutex_lock_iothread();
        kvm_arch_post_run(cpu, run);

        if (run_ret < 0) {
            if (run_ret == -EINTR || run_ret == -EAGAIN) {
                DPRINTF("io window exit\n");
                ret = EXCP_INTERRUPT;
                break;
            }
            fprintf(stderr, "error: kvm run failed %s\n",
                    strerror(-run_ret));
            ret = -1;
            break;
        }

        trace_kvm_run_exit(cpu->cpu_index, run->exit_reason);
        switch (run->exit_reason) {
        case KVM_EXIT_IO:
            DPRINTF("handle_io\n");
            kvm_handle_io(run->io.port,
                          (uint8_t *)run + run->io.data_offset,
                          run->io.direction,
                          run->io.size,
                          run->io.count);
            ret = 0;
            break;
        case KVM_EXIT_MMIO:
            DPRINTF("handle_mmio\n");
            cpu_physical_memory_rw(run->mmio.phys_addr,
                                   run->mmio.data,
                                   run->mmio.len,
                                   run->mmio.is_write);
            ret = 0;
            break;
        case KVM_EXIT_IRQ_WINDOW_OPEN:
            DPRINTF("irq_window_open\n");
            ret = EXCP_INTERRUPT;
            break;
        case KVM_EXIT_SHUTDOWN:
            DPRINTF("shutdown\n");
            qemu_system_reset_request();
            ret = EXCP_INTERRUPT;
            break;
        case KVM_EXIT_UNKNOWN:
            fprintf(stderr, "KVM: unknown exit, hardware reason %" PRIx64 "\n",
                    (uint64_t)run->hw.hardware_exit_reason);
            ret = -1;
            break;
        case KVM_EXIT_INTERNAL_ERROR:
            ret = kvm_handle_internal_error(cpu, run);
            break;
        case KVM_EXIT_SYSTEM_EVENT:
            switch (run->system_event.type) {
            case KVM_SYSTEM_EVENT_SHUTDOWN:
                qemu_system_shutdown_request();
                ret = EXCP_INTERRUPT;
                break;
            case KVM_SYSTEM_EVENT_RESET:
                qemu_system_reset_request();
                ret = EXCP_INTERRUPT;
                break;
            default:
                DPRINTF("kvm_arch_handle_exit\n");
                ret = kvm_arch_handle_exit(cpu, run);
                break;
            }
            break;
        default:
            DPRINTF("kvm_arch_handle_exit\n");
            ret = kvm_arch_handle_exit(cpu, run);
            break;
        }
    } while (ret == 0);

    if (ret < 0) {
        cpu_dump_state(cpu, stderr, fprintf, CPU_DUMP_CODE);
        vm_stop(RUN_STATE_INTERNAL_ERROR);
    }

    cpu->exit_request = 0;
    return ret;
}
```

同时`vcpu_run`函数的主体结构也是一个循环，首先判断当前CPU是否可运行，可运行就进入虚拟机，同时对vcpu->requests上的请求做处理，接下来是处理虚拟中断相关请求。

#### VCPU的调度

操作系统一般可以自由的将VCPU调度到任何一个物理CPU上运行。当VCPU在不同的物理CPU上运行的时候，会影响虚拟机的性能，因为在同一个物理CPU上运行VCPU时只需要执行VMRESUME指令即可，但是如果要切换到不同的物理CPU，则需要执行VMCLEAR，VMPTRLD和VMLAUNCH指令。

这里列举将VCPU调度到不同物理CPU的基本步骤：

1. 在源物理CPU执行VMCLEAR指令，这可以保证将当前CPU关联的VMCS相关缓存数据冲刷到内存中
2. 在目的VMCS区域以VCPU的VMCS物理地址为操作数执行VMPTRLD指令
3. 在目的VMCS区域执行VMLAUNCH指令

